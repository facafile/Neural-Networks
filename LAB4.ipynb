{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7454477,"sourceType":"datasetVersion","datasetId":4339011}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 7 Recurrent neural networks\nIn this exercise we will try a simple experiment with a recurrent neural network. One of the well-known recurrent neural network models is the so called Long short-term memory (LSTM) network. More information on LSTM can be found in the text [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n\n## 7.1 The MNIST dataset revisited (1)\nIn one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a recurrent neural network to the problem of digits classification. To keep it simple, we will use a simple LSTM network that will be fed with one row of the image at a time. With each new row, it will update its states and give its prediction. What we are interested in is its prediction after the last row i.e. after it has the full information.","metadata":{"id":"cphH-e6Mq3ji"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n# Constants\nlearning_rate = 0.001\nnum_epochs = 10\nbatch_size = 100\n#we will feed a row at a time to the LSTM and there are 28 rows per image\ntimesteps = 28\n#each row has 28 columns whose values are simultaneously passed to LSTM\nn_input = 28 # MNIST data input (img shape: 28*28)\n#the number of hidden states in the LSTM\nn_hidden = 128\nn_classes = 10\n\n# Data transformation\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define the LSTM model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Initialize the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LSTMModel(n_input, n_hidden, 1, n_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.view(-1, timesteps, n_input).to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Test the model\n    if (epoch + 1) % 1 == 0:\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                images = images.view(-1, timesteps, n_input).to(device)\n                labels = labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n            accuracy = 100 * correct / total\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')","metadata":{"id":"h69cwQIWq3jm","execution":{"iopub.status.busy":"2024-01-22T16:42:43.475559Z","iopub.execute_input":"2024-01-22T16:42:43.476012Z","iopub.status.idle":"2024-01-22T16:45:19.608614Z","shell.execute_reply.started":"2024-01-22T16:42:43.475961Z","shell.execute_reply":"2024-01-22T16:45:19.607650Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Epoch [1/10], Test Accuracy: 96.45%\nEpoch [2/10], Test Accuracy: 97.58%\nEpoch [3/10], Test Accuracy: 98.12%\nEpoch [4/10], Test Accuracy: 98.45%\nEpoch [5/10], Test Accuracy: 98.39%\nEpoch [6/10], Test Accuracy: 98.47%\nEpoch [7/10], Test Accuracy: 98.63%\nEpoch [8/10], Test Accuracy: 98.69%\nEpoch [9/10], Test Accuracy: 98.71%\nEpoch [10/10], Test Accuracy: 98.65%\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nlista = []\nwith torch.no_grad():\n        for i in range(1,29):\n            correct = 0\n            total = 0\n\n            for images, labels in test_loader:\n                    images = images.view(-1, timesteps, n_input).to(device)\n                    images = images[:,:i,:]\n                    labels = labels.to(device)\n                    outputs = model(images)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n\n            accuracy = 100 * correct / total\n            lista.append(accuracy)\n            #print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, rows = {i}')\n\nplt.plot(range(1,29),lista)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:31:10.819033Z","iopub.execute_input":"2024-01-22T16:31:10.819373Z","iopub.status.idle":"2024-01-22T16:32:13.055689Z","shell.execute_reply.started":"2024-01-22T16:31:10.819347Z","shell.execute_reply":"2024-01-22T16:32:13.054752Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Constants\nlearning_rate = 0.001\nnum_epochs = 10\nbatch_size = 100\n#we will feed a row at a time to the LSTM and there are 28 rows per image\ntimesteps = 28\n#each row has 28 columns whose values are simultaneously passed to LSTM\nn_input = 28 # MNIST data input (img shape: 28*28)\n#the number of hidden states in the LSTM\nn_hidden = 128\nn_classes = 10\n\n# Data transformation\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define the LSTM model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Initialize the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LSTMModel(n_input, n_hidden, 1, n_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n# Training loop\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.view(-1, timesteps, n_input).to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Test the model\nlista = []\nwith torch.no_grad():\n        for i in range(1,29):\n            correct = 0\n            total = 0\n\n            for images, labels in test_loader:\n                    images = images.view(-1, timesteps, n_input).to(device)\n                    images = images[:,:i,:]\n                    labels = labels.to(device)\n                    outputs = model(images)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n\n            accuracy = 100 * correct / total\n            lista.append(accuracy)\n            #print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, rows = {i}')\n\nplt.plot(range(1,29),lista)\nplt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:32:52.121450Z","iopub.execute_input":"2024-01-22T16:32:52.121860Z","iopub.status.idle":"2024-01-22T16:36:08.242212Z","shell.execute_reply.started":"2024-01-22T16:32:52.121814Z","shell.execute_reply":"2024-01-22T16:36:08.241152Z"},"trusted":true},"execution_count":75,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"using SGD the graph is not the same as with Adam optimiser, we see that the sgd gets stuck in some local minimum and with that we get much lower accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Tasks**\n\n1. Study and run the code above.\n2. Draw a plot that shows the relation between the number of rows given to the network and its final accuracy on the test set.\n3. What happens if we use gradient descent instead of Adam?\n","metadata":{"id":"iRbpYMdrq3jn"}},{"cell_type":"markdown","source":"# 8 Convolutional neural networks\n\n\nThe goal of this exercise is to learn the basic stuff about [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN or ConvNet). In the previous exercises, the building blocks mostly included simple operations that had some kind of activations functions and each layer was usually fully connected to the previous one. CNNs take into account the spatial nature of the input data, e.g. an image, and they process it by applying one or more  [kernels](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29). In the case of images, this processing i.e. convolving is also known as filtering. The results of processing the input with a single kernel will be a single channel, but usually a convolutional layer involves more kernels producing more channels. These channels are often called **feature maps** because each kernel is specialized for extraction of a certain kind of features from the input. These feature maps are then combined into a single tensor that can be viewed as an image with multiple channels that can be then passed to further convolutional layers.\n\nFor example, if the input consists of a grayscale image i.e. an image with only one channel and a $5\\times 5$ kernel is applied, the result is a single feature map. The borders of the input image are usually padded with zeros to ensure that the resulting feature maps has the same number of rows and columns as the input image.\n\nIf the input consists of a color image i.e. an image with three channels and a $5\\times 5$ kernel is applied, what will actually be applied is an $5\\times 5\\times 3$ kernel that will simultaneously process all three channels and the result will again be a single feature map. However, if e.g. 16 several kernels are applied, then the result will be 16 feature maps. Should they be passed to another convolutional layer, **each** of its kernels would simultaneously process **all** feature maps so their sizes would be e.g. $3\\times 3\\times 16$ or $5\\times 5\\times 16$ where 16 is used to reach all feature maps simultaneously.\n\nThe convolution is usually followed by applying an element-wise non-linear operation to each of the values in the feature maps. Finally, what often follows is the summarization i.e. pooling of the information in the feature maps to reduce the spatial dimensions and keep only the most important information. A common approach used here is the so called max pooling. It is a non-linear downsampling where the input is divided into a set of non-overlapping rectangles and for each of them only the maximum value inside of it is kept.\n\n![Model of a neuron](data:image/png;base64,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)\n<center>Figure 1. Max pooling with $2\\times 2$ rectangles (taken from [Wikipedia](https://en.wikipedia.org/wiki/File:Max_pooling.png)).</center>\n\nWhat usually follows after several convolutional layers is putting the values of all feature maps into a single vector, which is then passed further to fully connected or other kinds of layers.\n\nThe number of parameters in the convolutional depends on the number of feature maps and the sizes of the kernels. For example, if a convolutional layer with 32 kernels of nominal size $3\\times 3$ receives 16 feature maps on its input, it will require $16\\times 3\\times 3\\times 32+32$ where the last 32 parameters refer to the kernel biases.","metadata":{"id":"NNpp9Ppweg3Q"}},{"cell_type":"markdown","source":"### Google Colab preliminaries\n\nUpload the zipped cnn_img folder to Google Colab and ensure the paths in the notebook are adjusted accordingly.\n\nIf the notebook is not run on Google Colab, skip the following commands.","metadata":{"id":"Ms42xQeSVkOO"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive')","metadata":{"id":"Kmh4zv-2rzmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /content/cnn_img.zip","metadata":{"id":"j8Wz1GQ5ZUlZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.1 The MNIST dataset revisited (2)\nIn one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a convolutional neural network to the digit classification problem. We will use the following layers to build our model:\n\n* [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n* [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n* [torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n\nThe [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer has the same effect as the fully connected layer, a matrix multiplication that was used in the previous exercise with the MNIST dataset.\n\n**Tasks**\n\n1. Study and run the code below. How is the accuracy compared to the ones obtained in the previous exerises with MNIST?\n2. Try to change the number and size of convolutional and fully connected layers. What has the greatest impact on the accuracy? For each network architecture configuration calculate the number of trainable parameters.\n3. What happens to the accuracy if another [non-linearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) is used instead of ReLU? Experiment with at least two different activation functions.","metadata":{"id":"OJdpRk2gV9q4"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport tqdm\nimport pandas as pd\n","metadata":{"id":"m_1TWwNif8WE","execution":{"iopub.status.busy":"2024-01-22T13:10:03.846954Z","iopub.execute_input":"2024-01-22T13:10:03.847904Z","iopub.status.idle":"2024-01-22T13:10:04.162711Z","shell.execute_reply.started":"2024-01-22T13:10:03.847866Z","shell.execute_reply":"2024-01-22T13:10:04.161692Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu1 = nn.ReLU()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu2 = nn.ReLU()\n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n\n    self.relu3 = nn.ReLU()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.relu1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.relu2(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.relu3(x)\n\n    # Non-linearity\n    pred_logits = self.fc2(x)\n\n    return pred_logits\n","metadata":{"id":"NK79DP85edIb","execution":{"iopub.status.busy":"2024-01-22T13:10:08.859858Z","iopub.execute_input":"2024-01-22T13:10:08.860622Z","iopub.status.idle":"2024-01-22T13:10:08.873302Z","shell.execute_reply.started":"2024-01-22T13:10:08.860578Z","shell.execute_reply":"2024-01-22T13:10:08.872081Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, device, train_dataloader, optimizer, epoch):\n\n    model.train()\n\n    train_loss = 0.0\n\n    for batch in tqdm.tqdm(train_dataloader):\n\n      # Every data instance is an input image + label pair\n      images, labels = batch\n\n      # It is necessary to have both the model, and the data on the same device, either CPU or GPU, for the model to process data.\n      # Data on CPU and model on GPU, or vice-versa, will result in a Runtime error.\n      images, labels = images.to(device), labels.to(device)\n\n      # Zero your gradients for every batch\n      optimizer.zero_grad()\n\n      # Make predictions for this batch\n      pred_logits = model(images)\n\n      # Compute the loss\n      loss = loss_fn(pred_logits, labels)\n\n      # Calculates the backward gradients over the learning weights\n      loss.backward()\n\n      # Tells the optimizer to perform one learning step\n      # Adjust the model�s learning weights based on the observed gradients for this batch\n      optimizer.step()\n\n      train_loss += loss.item()\n\n    # Print epoch's average loss\n    print(\"Epoch {} - Training loss: {}\".format(epoch+1, train_loss/len(train_dataloader)))\n\n\ndef evaluation(model, device, test_dataloader, epoch):\n\n    # Sets layers like dropout and batch normalization to evaluation mode before running inference\n    # Failing to do this will yield inconsistent inference results\n    model.eval()\n\n    test_accuracy = 0.0\n\n    # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().\n    # It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n    with torch.no_grad():\n\n      for batch in tqdm.tqdm(test_dataloader):\n\n        images, labels = batch\n\n        images, labels = images.to(device), labels.to(device)\n\n        pred_logits = model(images)\n\n        probabilities = torch.nn.functional.softmax(pred_logits, dim=1)\n\n        # Find the index of the highest probability\n        predictions = probabilities.argmax(dim=1)\n\n        # Caluculate average batch accuracy\n        batch_accuracy = torch.mean((predictions == labels).float())\n\n        test_accuracy += batch_accuracy\n\n      print(\"Epoch {} - Accuracy: {}\".format(epoch+1, test_accuracy/len(test_dataloader)))\n\n\n","metadata":{"id":"FFJtOH6oNfLR","execution":{"iopub.status.busy":"2024-01-22T13:11:52.704693Z","iopub.execute_input":"2024-01-22T13:11:52.705154Z","iopub.status.idle":"2024-01-22T13:11:52.716582Z","shell.execute_reply.started":"2024-01-22T13:11:52.705122Z","shell.execute_reply":"2024-01-22T13:11:52.715673Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Move model to GPU if possible\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Augmentations\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n# Stores the samples and their corresponding labels\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\n# Wraps an iterable around the Dataset to enable easy access to the samples.\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n","metadata":{"id":"xpUS9zIEoZ4p","execution":{"iopub.status.busy":"2024-01-22T13:18:52.109676Z","iopub.execute_input":"2024-01-22T13:18:52.110504Z","iopub.status.idle":"2024-01-22T13:18:52.208649Z","shell.execute_reply.started":"2024-01-22T13:18:52.110471Z","shell.execute_reply":"2024-01-22T13:18:52.207730Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n\n","metadata":{"id":"dPXNNzNl7Lzb","execution":{"iopub.status.busy":"2024-01-22T13:12:04.641825Z","iopub.execute_input":"2024-01-22T13:12:04.642698Z","iopub.status.idle":"2024-01-22T13:13:36.051396Z","shell.execute_reply.started":"2024-01-22T13:12:04.642666Z","shell.execute_reply":"2024-01-22T13:13:36.049999Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:16<00:00, 56.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.1176562375717435\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 71.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9882563948631287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:16<00:00, 58.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.03915885403542085\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 71.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9883559346199036\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 58.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.027547722017529783\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9926353693008423\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.01988373714599891\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 71.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9915406107902527\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:16<00:00, 58.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.016213571712715244\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.26it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9906449317932129\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"the accuracy is a little bit higher than in the previous model","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu1 = nn.ReLU()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2 * 2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu2 = nn.ReLU()\n    \n    self.conv4 = nn.Conv2d(in_channels=n_channels_2 * 2,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n    self.relu5 = nn.ReLU()\n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected * 2, bias=True)\n\n    self.relu3 = nn.ReLU()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected * 2, out_features=n_fully_connected, bias=True)\n    \n    self.relu4 = nn.ReLU()\n    \n    \n    self.fc3 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.relu1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.relu2(x)\n    \n    x = self.conv4(x)\n    \n    x = self.relu5(x)\n            \n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.relu3(x)\n\n    # Non-linearity\n    x = self.fc2(x)\n    \n    x = self.relu4(x)\n    \n    pred_logits = self.fc3(x)\n\n    return pred_logits\n\n\n\n\n# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Move model to GPU if possible\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Augmentations\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n# Stores the samples and their corresponding labels\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\n# Wraps an iterable around the Dataset to enable easy access to the samples.\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n    \npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"num of trainable parameters = {pytorch_total_params}\")\n\nprint(\"changing the parameters\")\ninput_channels=1\nn_channels_1=16\nn_channels_2=32\nn_classes=10\nn_fully_connected=64\nkernel_size=5\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n    \npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"num of trainable parameters = {pytorch_total_params}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:22:40.026569Z","iopub.execute_input":"2024-01-22T14:22:40.027497Z","iopub.status.idle":"2024-01-22T14:26:22.086972Z","shell.execute_reply.started":"2024-01-22T14:22:40.027460Z","shell.execute_reply":"2024-01-22T14:26:22.085993Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 47.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.13117370409019757\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9885549545288086\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 46.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.04377839399104777\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9926353693008423\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:20<00:00, 46.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.027855444561018905\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 70.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9898487329483032\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 46.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.023557392431596418\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 69.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9880573153495789\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 47.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.018977914877256027\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9847730994224548\nnum of trainable parameters = 1145482\nchanging the parameters\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:20<00:00, 46.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.02038801389295607\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9898487329483032\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 47.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.01459491070864678\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9895501732826233\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:20<00:00, 46.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.014785271820221697\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9932324886322021\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:20<00:00, 46.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.011446487805747594\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 70.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9906449317932129\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:20<00:00, 46.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.009376963653936529\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.83it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.993332028388977\nnum of trainable parameters = 1145482\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n    \n    self.conv4 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1 * 2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n    self.relu5 = nn.ReLU()\n\n    self.conv1 = nn.Conv2d(in_channels=n_channels_1 * 2,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu1 = nn.ReLU()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu2 = nn.ReLU()\n    \n   \n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected * 2, bias=True)\n\n    self.relu3 = nn.ReLU()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected * 2, out_features=n_fully_connected, bias=True)\n    \n    self.relu4 = nn.ReLU()\n    \n    \n    self.fc3 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n        \n    x = self.conv4(x)\n    \n    x = self.relu5(x)\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.relu1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.relu2(x)\n    \n    \n            \n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.relu3(x)\n\n    # Non-linearity\n    x = self.fc2(x)\n    \n    x = self.relu4(x)\n    \n    pred_logits = self.fc3(x)\n\n    return pred_logits\n\n\n\n\n# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Move model to GPU if possible\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Augmentations\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n# Stores the samples and their corresponding labels\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\n# Wraps an iterable around the Dataset to enable easy access to the samples.\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n    \npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"num of trainable parameters = {pytorch_total_params}\")\n\nprint(\"changing the parameters\")\ninput_channels=1\nn_channels_1=16\nn_channels_2=32\nn_classes=10\nn_fully_connected=64\nkernel_size=5\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n    \npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"num of trainable parameters = {pytorch_total_params}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:28:40.935894Z","iopub.execute_input":"2024-01-22T14:28:40.936395Z","iopub.status.idle":"2024-01-22T14:32:15.951161Z","shell.execute_reply.started":"2024-01-22T14:28:40.936366Z","shell.execute_reply":"2024-01-22T14:32:15.950171Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.12498854881357044\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9859673976898193\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.03881947197767073\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 71.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9890525341033936\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.02824287545382816\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 71.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.990545392036438\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.022333046136827954\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 70.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9901472926139832\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.019131987027529867\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 68.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9910430312156677\nnum of trainable parameters = 941418\nchanging the parameters\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.018683298520944094\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9911425113677979\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.013143417766530902\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9918391704559326\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.012896155422408608\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 69.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.993332028388977\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.011051220896559176\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 71.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9901472926139832\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:19<00:00, 48.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.012814764162610664\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.16it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.990545392036438\nnum of trainable parameters = 941418\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"we see that both adding layer to the network and changing the parameters result in slightly different accuracy but it seems that the hiperparameters that control the size of layers is more important than adding a few more layers to a already complex enough network for this easy problem of classification.","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu1 = nn.Sigmoid()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu2 = nn.Sigmoid()\n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n\n    self.relu3 = nn.Sigmoid()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.relu1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.relu2(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.relu3(x)\n\n    # Non-linearity\n    pred_logits = self.fc2(x)\n\n    return pred_logits\n\n# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Move model to GPU if possible\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Augmentations\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n# Stores the samples and their corresponding labels\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\n# Wraps an iterable around the Dataset to enable easy access to the samples.\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:39:00.768537Z","iopub.execute_input":"2024-01-22T14:39:00.769450Z","iopub.status.idle":"2024-01-22T14:40:29.793319Z","shell.execute_reply.started":"2024-01-22T14:39:00.769414Z","shell.execute_reply":"2024-01-22T14:40:29.792340Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.6157216931353691\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9634753465652466\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.09397086561290122\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9800955653190613\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.06003357456879877\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9858678579330444\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.04575126534968273\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9862659573554993\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.03640586966864649\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.03it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9876592755317688\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu1 = nn.Tanh()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu2 = nn.Tanh()\n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n\n    self.relu3 = nn.Tanh()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.relu1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.relu2(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.relu3(x)\n\n    # Non-linearity\n    pred_logits = self.fc2(x)\n\n    return pred_logits\n\n# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Move model to GPU if possible\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Augmentations\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n# Stores the samples and their corresponding labels\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\n# Wraps an iterable around the Dataset to enable easy access to the samples.\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:40:53.126953Z","iopub.execute_input":"2024-01-22T14:40:53.127790Z","iopub.status.idle":"2024-01-22T14:42:21.437206Z","shell.execute_reply.started":"2024-01-22T14:40:53.127756Z","shell.execute_reply":"2024-01-22T14:42:21.436110Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.12568811774611283\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9827826619148254\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.043570799026032614\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9873606562614441\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.031468562239015015\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9891520738601685\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.027400437062348067\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9837778806686401\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.022947059758199898\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.26it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9891520738601685\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I used 2 more models, each having different act. function. One has Sigmoid and the other tanh. Both model gave accuracy below 99% wich is lower than the accuracy model with Relu gave me.","metadata":{}},{"cell_type":"markdown","source":"## 8.2 Image classification\nImage classification is a challenging computer vision problem with the best-known competition being [The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/), which includes the ImageNet dataset with millions of $224\\times 224$ training images. The class names in one of the tasks there can be found [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). One of the most important breakthroughs was when in 2012 the convolutional neural network [AlexNet](https://en.wikipedia.org/wiki/AlexNet) won the first place. Ever since many highly successful convolutional neural networks architectures have been proposed, e.g. [VGG-16](https://arxiv.org/abs/1409.1556), [VGG-19](https://arxiv.org/abs/1409.1556), [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), [Inception](https://arxiv.org/abs/1409.4842), etc. Training such networks requires a lot of time because they have many layers with millions of parameters. In this exercise we are going to experiment with pre-trained models of some of the best known architectures.\n\n### 8.2.1 Using pre-trained models\nTry running the following code:","metadata":{"id":"EsU9MOc8QmDe"}},{"cell_type":"code","source":"import torchvision.models as models\nfrom torchvision.io import read_image\n\n### Choose the architecture\narchitecture=\"resnet34\"\n#architecture=\"vgg16\"\n#architecture=\"vgg19\"\n#architecture=\"inceptionv3\"\n\nlista = [\"resnet34\", \"vgg16\", \"vgg19\", \"inceptionv3\"]\n\nfor i in range(len(lista)):\n    print(lista[i])\n    if lista[i] == \"resnet34\":\n      weights = models.ResNet34_Weights.DEFAULT\n      model = models.resnet34(weights=weights)\n    elif lista[i] == \"vgg16\":\n      weights = models.VGG16_Weights.DEFAULT\n      model = models.vgg16(pretrained=weights)\n    elif lista[i] == \"vgg19\":\n      weights = models.VGG19_Weights.DEFAULT\n      model = models.vgg19(pretrained=weights)\n    elif lista[i] == \"inceptionv3\":\n      weights = models.Inception_V3_Weights.DEFAULT\n      model = models.inception_v3(pretrained=weights)\n\n    model.eval()\n\n    image_paths=[\"/kaggle/input/cnn-img/cnn_img/badger.jpg\", \"/kaggle/input/cnn-img/cnn_img/rabbit.jpg\", \"/kaggle/input/cnn-img/cnn_img/sundial.jpg\", \"/kaggle/input/cnn-img/cnn_img/pineapple.jpg\", \"/kaggle/input/cnn-img/cnn_img/can.jpg\"]\n\n    for path in image_paths:\n        #loading the image and rescaling it to fit the size for the imagenet architectures\n        img = read_image(path)\n        preprocess = weights.transforms(antialias=True)\n        batch = preprocess(img).unsqueeze(0)\n\n        prediction = model(batch).squeeze(0).softmax(0)\n        class_id = prediction.argmax().item()\n        score = prediction[class_id].item()\n        category_name = weights.meta[\"categories\"][class_id]\n        \n        print(f\"{category_name}: {100 * score:.1f}%\")","metadata":{"id":"qOJamRVpQrdo","execution":{"iopub.status.busy":"2024-01-22T14:46:50.904102Z","iopub.execute_input":"2024-01-22T14:46:50.904786Z","iopub.status.idle":"2024-01-22T14:46:57.777292Z","shell.execute_reply.started":"2024-01-22T14:46:50.904752Z","shell.execute_reply":"2024-01-22T14:46:57.776369Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"resnet34\nbadger: 99.5%\nwood rabbit: 89.4%\nsundial: 100.0%\npineapple: 100.0%\ntable lamp: 38.9%\nvgg16\nbadger: 100.0%\nwood rabbit: 98.9%\nsundial: 100.0%\npineapple: 97.9%\nvase: 15.9%\nvgg19\nbadger: 100.0%\nwood rabbit: 97.3%\nsundial: 100.0%\npineapple: 99.8%\nashcan: 13.0%\ninceptionv3\nbadger: 95.4%\nwood rabbit: 90.6%\nsundial: 85.4%\npineapple: 99.3%\ntable lamp: 17.2%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"there are some significant differences between accuracy in terms of provided pictures and using different networks as we can see from the result above. the last item is not the same prediction in all models.","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\nfrom torchvision.io import read_image\n\n### Choose the architecture\narchitecture=\"resnet34\"\n#architecture=\"vgg16\"\n#architecture=\"vgg19\"\n#architecture=\"inceptionv3\"\n\nlista = [\"resnet34\", \"vgg16\", \"vgg19\", \"inceptionv3\"]\nimage_paths=[\"/kaggle/input/cnn-img/cnn_img/healthy/0.jpg\", \"/kaggle/input/cnn-img/cnn_img/healthy/35.jpg\", \"/kaggle/input/cnn-img/cnn_img/healthy/28.jpg\", \"/kaggle/input/cnn-img/cnn_img/unhealthy/0.jpg\",\"/kaggle/input/cnn-img/cnn_img/unhealthy/21.jpg\"]\n\nfor i in range(len(lista)):\n    print(lista[i])\n    if lista[i] == \"resnet34\":\n      weights = models.ResNet34_Weights.DEFAULT\n      model = models.resnet34(weights=weights)\n    elif lista[i] == \"vgg16\":\n      weights = models.VGG16_Weights.DEFAULT\n      model = models.vgg16(pretrained=weights)\n    elif lista[i] == \"vgg19\":\n      weights = models.VGG19_Weights.DEFAULT\n      model = models.vgg19(pretrained=weights)\n    elif lista[i] == \"inceptionv3\":\n      weights = models.Inception_V3_Weights.DEFAULT\n      model = models.inception_v3(pretrained=weights)\n\n    model.eval()\n\n    \n    for path in image_paths:\n        #loading the image and rescaling it to fit the size for the imagenet architectures\n        img = read_image(path)\n        preprocess = weights.transforms(antialias=True)\n        batch = preprocess(img).unsqueeze(0)\n\n        prediction = model(batch).squeeze(0).softmax(0)\n        class_id = prediction.argmax().item()\n        score = prediction[class_id].item()\n        category_name = weights.meta[\"categories\"][class_id]\n        \n        print(f\"{category_name}: {100 * score:.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:56:43.413895Z","iopub.execute_input":"2024-01-22T14:56:43.414312Z","iopub.status.idle":"2024-01-22T14:56:50.285461Z","shell.execute_reply.started":"2024-01-22T14:56:43.414281Z","shell.execute_reply":"2024-01-22T14:56:50.284265Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"resnet34\nhot pot: 84.6%\nGranny Smith: 22.5%\ntrifle: 61.4%\npotpie: 29.1%\npizza: 97.6%\nvgg16\ncorn: 49.8%\nlemon: 13.4%\ntrifle: 96.5%\ncarbonara: 21.2%\npizza: 98.6%\nvgg19\ncorn: 23.9%\nfig: 12.2%\ntrifle: 93.9%\nplate: 22.9%\npizza: 94.0%\ninceptionv3\ncorn: 36.4%\nhamper: 26.6%\ntrifle: 57.1%\ncheeseburger: 35.3%\npizza: 94.0%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The pizza and trifle classes have a high probability and are correctly classified as for the picture with fruit each model picks its fruit and the french fries picture is also classified wrong.","metadata":{}},{"cell_type":"markdown","source":"**Tasks**\n1. Is there any significant difference between the results of different architectures?\n2. Try to classify several other images from the folders cnn_img/healthy and cnn_img/unhealthy that you choose on your own. Which cases are problematic?\n\n### 8.2.2 Creating your own classifier - pincers vs. scissors\nAlthough ImageNet has a lot of classes, sometimes they do not cover some desired cases. Let's assume that we want to tell images with pincers apart from the ones with scissors. Neither pincers nor scissors are among ImageNet classes. Nevertheless, we can still use some parts of the pre-trained models.\n\nVarious layers of a deep convolutional network have diferent tasks. The ones closest to the original input image usually look for features such as edges and corners i.e. for low-level features. After them there are layers that look for middle-level features such as circular objects, special curves, etc. Next, there are usually fully connected layers that create high-level semantic features by combining the information from the previous layers. These features are then used by the last layer that performs the actual classification. What we can do here is simply to discard the last layer i.e. not to calculate the class of an image, but to extract the values in on of the fully connected layers. This effectively means that we are going to use the network only as an extractor for high-level features that we would hardly be able to engineer on our own. Let's first see which layers can be found in the ResNet network:","metadata":{"id":"eZmSyBglaUoa"}},{"cell_type":"code","source":"import torchvision.models as models\nimport numpy as np\n\narchitecture=\"vgg16\"\n\nif architecture == \"resnet34\":\n  weights = models.ResNet34_Weights.DEFAULT\n  base_model = models.resnet34(weights=weights)\nelif architecture == \"resnet50\":\n  weights = models.ResNet50_Weights.DEFAULT\n  base_model = models.resnet50(weights=weights)\nelif architecture == \"vgg16\":\n  weights = models.VGG16_Weights.DEFAULT\n  base_model = models.vgg16(weights=weights)\n\nfor layer in base_model.children():\n    print(layer)","metadata":{"id":"ztcpyruTRTJD","execution":{"iopub.status.busy":"2024-01-22T15:40:51.416185Z","iopub.execute_input":"2024-01-22T15:40:51.417146Z","iopub.status.idle":"2024-01-22T15:40:52.894656Z","shell.execute_reply.started":"2024-01-22T15:40:51.417112Z","shell.execute_reply":"2024-01-22T15:40:52.893675Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nAdaptiveAvgPool2d(output_size=(7, 7))\nSequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"At the end you can see fully connected layer used for classification. We can extract the values from previous layers by using the following code:","metadata":{"id":"acvV9u06GocO"}},{"cell_type":"code","source":"# Model without last fully connected layer\nmodel = torch.nn.Sequential(*(list(base_model.children())[:-1]), nn.AdaptiveAvgPool2d(1))\n\nimg_path=\"/kaggle/input/cnn-img/cnn_img/rabbit.jpg\"\n\nimg = read_image(img_path)\npreprocess = weights.transforms(antialias=True)\nbatch = preprocess(img).unsqueeze(0)\n\nfeatures = model(batch).squeeze(3).squeeze(2)\n\nprint(features.shape)\nfeature_layer_size=features.shape[1];","metadata":{"id":"Gxut2qMPJS3Y","execution":{"iopub.status.busy":"2024-01-22T15:23:08.894182Z","iopub.execute_input":"2024-01-22T15:23:08.895069Z","iopub.status.idle":"2024-01-22T15:23:08.984743Z","shell.execute_reply.started":"2024-01-22T15:23:08.895034Z","shell.execute_reply":"2024-01-22T15:23:08.983804Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"torch.Size([1, 512])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"These values can now be used as features and that can later be used with another classifier. Let's first extract the features for our pincer and scissors images.","metadata":{"id":"VjTMbaFPKfvk"}},{"cell_type":"code","source":"def create_numbered_paths(home_dir, n):\n    return [home_dir+str(i)+\".jpg\" for i in range(n)]\n\ndef create_paired_numbered_paths(first_home_dir, second_home_dir, n):\n    image_paths=[]\n    for p in zip(create_numbered_paths(first_home_dir, n), create_numbered_paths(second_home_dir, n)):\n        image_paths.extend(p)\n    return image_paths\n\ndef create_features(paths, verbose=True):\n    n=len(paths)\n    features=np.zeros((n, feature_layer_size))\n    for i in range(n):\n        if (verbose==True):\n            print(\"\\t%2d / %2d\"%(i+1, n))\n        img = read_image(paths[i])\n        preprocess = weights.transforms(antialias=True)\n        batch = preprocess(img).unsqueeze(0)\n        features[i, :]=model(batch).squeeze(3).squeeze(2).detach().numpy()\n\n    return features\n\npincers_dir=\"/kaggle/input/cnn-img/cnn_img/pincers/\"\nscissors_dir=\"/kaggle/input/cnn-img/cnn_img/scissors/\"\n\nindividual_n=50\n\n#combining all image paths\nimage_paths=create_paired_numbered_paths(pincers_dir, scissors_dir, individual_n)\n\n#marking their classes\nimage_classes=[]\nfor i in range(individual_n):\n    #0 stands for the pincer image and 0 stands for the scissors image\n    image_classes.extend((0, 1))\n\n#number of all images\nn=100\n#number of training images\nn_train=50\n#number of test images\nn_test=n-n_train\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n\n#train classes\ny_test=np.array(image_classes[n_train:])","metadata":{"id":"WPNycPJ4KhBQ","execution":{"iopub.status.busy":"2024-01-22T15:25:42.189023Z","iopub.execute_input":"2024-01-22T15:25:42.189467Z","iopub.status.idle":"2024-01-22T15:25:50.198601Z","shell.execute_reply.started":"2024-01-22T15:25:42.189434Z","shell.execute_reply":"2024-01-22T15:25:50.197600Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Creating training features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\nCreating test features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that for each image we have its features, we will divide the images into a training and a test set. Then we will use a linear SVM classifier to classify them.","metadata":{"id":"nQQp23pwweEf"}},{"cell_type":"code","source":"from sklearn import svm\n\ndef create_svm_classifier(x, y, C=1.0, kernel='linear'):\n    #we will use linear SVM\n    classifier=svm.SVC(kernel=kernel, C=C);\n    classifier.fit(x, y)\n    return classifier\n\ndef calculate_accuracy(classifier, x, y):\n    predicted=classifier.predict(x)\n    return np.sum(y==predicted)/y.size","metadata":{"id":"R12r9KSrgt95","execution":{"iopub.status.busy":"2024-01-22T15:26:16.252101Z","iopub.execute_input":"2024-01-22T15:26:16.252966Z","iopub.status.idle":"2024-01-22T15:26:16.840508Z","shell.execute_reply.started":"2024-01-22T15:26:16.252892Z","shell.execute_reply":"2024-01-22T15:26:16.839449Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#training the model\nclassifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, 1.50, \"rbf\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, 2.10, \"linear\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, .50, \"rbf\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))","metadata":{"id":"ADOM91TWwdmS","execution":{"iopub.status.busy":"2024-01-22T15:35:41.086313Z","iopub.execute_input":"2024-01-22T15:35:41.086666Z","iopub.status.idle":"2024-01-22T15:35:41.106979Z","shell.execute_reply.started":"2024-01-22T15:35:41.086639Z","shell.execute_reply":"2024-01-22T15:35:41.106105Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Accuracy: 96.00%\nAccuracy: 96.00%\nAccuracy: 96.00%\nAccuracy: 98.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"i got better accuracy on the model only when i used a simpler model with more regularisation, but making more complex models did not change the accuracy.","metadata":{}},{"cell_type":"code","source":"# Model without last fully connected layer\nmodel = torch.nn.Sequential(*(list(base_model.children())[:-1]), nn.AdaptiveAvgPool2d(1))\n\nimg_path=\"/kaggle/input/cnn-img/cnn_img/rabbit.jpg\"\n\nimg = read_image(img_path)\npreprocess = weights.transforms(antialias=True)\nbatch = preprocess(img).unsqueeze(0)\n\nfeatures = model(batch).squeeze(3).squeeze(2)\n\nprint(features.shape)\nfeature_layer_size=features.shape[1];\n\n\n\ndef create_numbered_paths(home_dir, n):\n    return [home_dir+str(i)+\".jpg\" for i in range(n)]\n\ndef create_paired_numbered_paths(first_home_dir, second_home_dir, n):\n    image_paths=[]\n    for p in zip(create_numbered_paths(first_home_dir, n), create_numbered_paths(second_home_dir, n)):\n        image_paths.extend(p)\n    return image_paths\n\ndef create_features(paths, verbose=True):\n    n=len(paths)\n    features=np.zeros((n, feature_layer_size))\n    for i in range(n):\n        if (verbose==True):\n            print(\"\\t%2d / %2d\"%(i+1, n))\n        img = read_image(paths[i])\n        preprocess = weights.transforms(antialias=True)\n        batch = preprocess(img).unsqueeze(0)\n        features[i, :]=model(batch).squeeze(3).squeeze(2).detach().numpy()\n\n    return features\n\npincers_dir=\"/kaggle/input/cnn-img/cnn_img/pincers/\"\nscissors_dir=\"/kaggle/input/cnn-img/cnn_img/scissors/\"\n\nindividual_n=50\n\n#combining all image paths\nimage_paths=create_paired_numbered_paths(pincers_dir, scissors_dir, individual_n)\n\n#marking their classes\nimage_classes=[]\nfor i in range(individual_n):\n    #0 stands for the pincer image and 0 stands for the scissors image\n    image_classes.extend((0, 1))\n\n#number of all images\nn=100\n#number of training images\nn_train=50\n#number of test images\nn_test=n-n_train\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n\n#train classes\ny_test=np.array(image_classes[n_train:])\n\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, 1.50, \"rbf\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, 2.10, \"linear\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, .50, \"rbf\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:41:59.386709Z","iopub.execute_input":"2024-01-22T15:41:59.387107Z","iopub.status.idle":"2024-01-22T15:42:17.427504Z","shell.execute_reply.started":"2024-01-22T15:41:59.387079Z","shell.execute_reply":"2024-01-22T15:42:17.426256Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"torch.Size([1, 512])\nCreating training features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\nCreating test features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\nAccuracy: 100.00%\nAccuracy: 100.00%\nAccuracy: 100.00%\nAccuracy: 98.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"when using vgg16 as the back bone we can achive accuracy of 100%","metadata":{}},{"cell_type":"markdown","source":"**Tasks**\n\n1. Is there any significant gain if more complex SVM models are used?\n2. What happens if we extract features using different backbone, e.g. vgg16?\n\n\n### 8.2.1 Creating your own classifier - healthy vs. unhealthy food\nThe previous example was relatively simple because all images were of same size and each of them had a white background, which allowed the extractor to concentrate only on the features of the actual objects. In this example we will use a slightly more complicated case - namely, will will tell images with healthy food apart from the ones with unhealthy food. FIrst let's repeat the same process as we did in the previous example and create the features:","metadata":{"id":"nSMhoT6swkeZ"}},{"cell_type":"code","source":"healthy_dir=\"/kaggle/input/cnn-img/cnn_img/healthy/\"\nunhealthy_dir=\"/kaggle/input/cnn-img/cnn_img/unhealthy/\"\n\nindividual_n=100\n\n#combining all image paths\nimage_paths=create_paired_numbered_paths(healthy_dir, unhealthy_dir, individual_n)\n\n#marking their classes\nimage_classes=[]\nfor i in range(individual_n):\n    #0 stands for the pincer image and 0 stands for the scissors image\n    image_classes.extend((0, 1))\n\n#number of all images\nn=200\n#number of training images\nn_train=100\n#number of test images\nn_test=n-n_train\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n#train classes\ny_test=np.array(image_classes[n_train:])","metadata":{"id":"JI75TDrVxNrR","execution":{"iopub.status.busy":"2024-01-22T15:44:04.477850Z","iopub.execute_input":"2024-01-22T15:44:04.478705Z","iopub.status.idle":"2024-01-22T15:44:41.736671Z","shell.execute_reply.started":"2024-01-22T15:44:04.478673Z","shell.execute_reply":"2024-01-22T15:44:41.735475Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Creating training features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\nCreating test features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's train a model and test its accuracy:","metadata":{"id":"pfezje2qxQns"}},{"cell_type":"code","source":"classifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))","metadata":{"id":"TXUOBrkjxNzU","execution":{"iopub.status.busy":"2024-01-22T15:44:49.370666Z","iopub.execute_input":"2024-01-22T15:44:49.371604Z","iopub.status.idle":"2024-01-22T15:44:49.380905Z","shell.execute_reply.started":"2024-01-22T15:44:49.371567Z","shell.execute_reply":"2024-01-22T15:44:49.379973Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Accuracy: 92.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"weights = models.ResNet50_Weights.DEFAULT\nbase_model = models.resnet50(weights=weights)\n\n\nhealthy_dir=\"/kaggle/input/cnn-img/cnn_img/healthy/\"\nunhealthy_dir=\"/kaggle/input/cnn-img/cnn_img/unhealthy/\"\n\nindividual_n=100\n\n#combining all image paths\nimage_paths=create_paired_numbered_paths(healthy_dir, unhealthy_dir, individual_n)\n\n#marking their classes\nimage_classes=[]\nfor i in range(individual_n):\n    #0 stands for the pincer image and 0 stands for the scissors image\n    image_classes.extend((0, 1))\n\n#number of all images\nn=200\n#number of training images\nn_train=100\n#number of test images\nn_test=n-n_train\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n#train classes\ny_test=np.array(image_classes[n_train:])\n\nclassifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:53:24.462267Z","iopub.execute_input":"2024-01-22T15:53:24.462688Z","iopub.status.idle":"2024-01-22T15:54:00.772359Z","shell.execute_reply.started":"2024-01-22T15:53:24.462653Z","shell.execute_reply":"2024-01-22T15:54:00.771343Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Creating training features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\nCreating test features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\nAccuracy: 93.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Tasks**\n1. Try the whole food classification with another network as a feature extractor and compare their results.\n2. What kind of test images are problematic?","metadata":{"id":"U-Zt5OzoxWqj"}},{"cell_type":"markdown","source":"when using resnet50 as a feature extractor we get 1% higher accuracy than with resnet34 and vgg16.","metadata":{}},{"cell_type":"markdown","source":"the problematic images are those that have a lot of redundant data which we do not need for our classification. If there are for example multiple objects on a picture such as fruit and we have classes lemons and oragnes that could potentialy make it harder to classify that image correctly. Also if there are pictures with different lighting than the ones we used in the train set since it can be harder to correctly extract features such as edges if we have bad lighting and so on.","metadata":{}}]}